
# -*- coding: utf-8 -*-
"""
src/preprocessing/score_and_export.py

ì—­í• :
- config.USE_TORCH í”Œë˜ê·¸ ë˜ëŠ” ì‹¤í–‰ ëª¨ë“œì— ë”°ë¼ ì„ íƒì ìœ¼ë¡œ ì ìˆ˜í™”
  1) USE_TORCH=True â†’ Torch ëª¨ë¸(.pt), ì‹¤íŒ¨ ì‹œ Sklearn(joblib) â†’ íœ´ë¦¬ìŠ¤í‹±
  2) USE_TORCH=False â†’ Sklearn(joblib), ì‹¤íŒ¨ ì‹œ íœ´ë¦¬ìŠ¤í‹±
- ì…ë ¥: embedding_text í¬í•¨ DF/CSV
- ì¶œë ¥: active/culture/nature/relaxation ì»¬ëŸ¼ ì¶”ê°€ + ì €ì¥ ì˜µì…˜

ì‚¬ìš© ì˜ˆ:
python -m src.preprocessing.score_and_export \
  --input ./data/raw/tour_raw_with_texts.csv \
  --output ./data/processed/tour_scored.csv
"""

import os, warnings, argparse
from typing import Union, Optional, List, Dict
import numpy as np
import pandas as pd
from src.config import RAW_WITH_TEXTS_CSV, SCORED_CSV, MODELS_DIR, USE_TORCH
from pathlib import Path
from sentence_transformers import SentenceTransformer
import torch

def _softmax_rows(X: np.ndarray, eps: float = 1e-9) -> np.ndarray:
    X = np.asarray(X, dtype=np.float64)
    X = X - X.max(axis=1, keepdims=True)
    ex = np.exp(X)
    return ex / (ex.sum(axis=1, keepdims=True) + eps)

# íœ´ë¦¬ìŠ¤í‹± í‚¤ì›Œë“œ(ê°„ë‹¨)
KEYWORDS: Dict[str, List[str]] = {
    "active":  ["ë“±ì‚°","íŠ¸ë ˆí‚¹","í•˜ì´í‚¹","ìº í•‘","ì„œí•‘","ë§ˆë¼í†¤","ìì „ê±°","íŒ¨ëŸ¬ê¸€ë¼ì´ë”©","ë˜í”„íŒ…","ì²´í—˜","ë ˆí¬ì¸ ","ìŠ¤í¬ì¸ ","í´ë¼ì´ë°","ì•”ë²½","ì‚°í–‰","ëŸ¬ë‹","ìŠ¹ë§ˆ","ì¹´ì•½","ì¹´ëˆ„","ì„œë°”ì´ë²Œ"],
    "culture": ["ë°•ë¬¼ê´€","ë¯¸ìˆ ê´€","ì „ì‹œ","ì „í†µ","ì—­ì‚¬","ê¶ê¶","ê³ ê¶","ì‚¬ì°°","ìœ ì ","ê¸°ë…ê´€","ê³µì—°","ì—°ê·¹","ë®¤ì§€ì»¬","ë¬¸í™”ì¬","ì˜ˆìˆ ","ì•„íŠ¸","ë¬¸í™”ì„¼í„°","ë„ì„œê´€","ì•„ì¹´ì´ë¸Œ","ì¶•ì œ","í˜ìŠ¤í‹°ë²Œ"],
    "nature":  ["ì‚°","ê³µì›","ë°”ë‹¤","í•´ë³€","í˜¸ìˆ˜","ê°•","ê³„ê³¡","ìˆ²","ì •ì›","ìŠµì§€","í­í¬","í•´ì•ˆ","ë‘˜ë ˆê¸¸","ìì—°","í•˜ì²œ","ì‚°ì±…ë¡œ","ì´ˆì›","êµ­ë¦½ê³µì›","ìì—°ê³µì›","ìƒíƒœê³µì›","ê°¯ë²Œ"],
    "relax":   ["ìŠ¤íŒŒ","ì˜¨ì²œ","ë¦¬ì¡°íŠ¸","íœ´ì–‘ë¦¼","íë§","ëª…ìƒ","ìš”ê°€","ì°œì§ˆë°©","í˜¸í…”","ì¹´í˜","ì •ì› ì‚°ì±…","íœ´ì‹","ì•¼ê²½","ì „ë§ëŒ€","í˜¸ìº‰ìŠ¤","ê¸€ë¨í•‘","í’€ë¹Œë¼","ë¦¬íŠ¸ë¦¿","ì‰¼í„°","íœ´ì–‘"],
}
def _heuristic_scores(texts: List[str]) -> np.ndarray:
    scores = []
    for t in texts:
        t0 = (t or "").lower()
        def any_in(keys): return any(k.lower() in t0 for k in keys)
        v = np.array([1.0 if any_in(KEYWORDS[k]) else 0.0 for k in ["active","culture","nature","relax"]], dtype=np.float64)
        if v.sum() == 0: v[:] = 0.25
        scores.append(v)
    return _softmax_rows(np.vstack(scores))

def _find_torch_model() -> Optional[str]:
    names = ["theme_regressor.pt", "theme_regressor_best.pt"]
    cands = [Path(MODELS_DIR), Path.cwd(), Path.cwd()/ "models"]
    for d in cands:
        for n in names:
            p = d / n
            if p.exists(): return str(p)
    return None

class TorchPredictor:
    def __init__(self, ckpt_path: str):
        self.ckpt = torch.load(ckpt_path, map_location="cpu")
        self.sbert_name = self.ckpt.get("sbert_name","sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
        self.enc = SentenceTransformer(self.sbert_name)
        in_dim = self.ckpt.get("in_dim", self.enc.encode(["x"], normalize_embeddings=True, convert_to_numpy=True).shape[1])
        hidden = self.ckpt.get("hidden", 512); dropout = self.ckpt.get("dropout", 0.1)
        import torch.nn as nn
        self.model = nn.Sequential(
            nn.Linear(in_dim, hidden), nn.ReLU(), nn.Dropout(dropout),
            nn.Linear(hidden, hidden//2), nn.ReLU(), nn.Dropout(dropout),
            nn.Linear(hidden//2, 4), nn.LogSoftmax(dim=-1)
        )
        self.model.load_state_dict(self.ckpt["state_dict"])
        self.model.eval()

    @torch.no_grad()
    def predict(self, texts: List[str]) -> np.ndarray:
        X = self.enc.encode(texts, normalize_embeddings=True, convert_to_numpy=True)
        B = 1024; out = []
        for i in range(0, len(X), B):
            xb = torch.from_numpy(X[i:i+B]).float()
            logp = self.model(xb); p = torch.exp(logp).cpu().numpy()
            out.append(p)
        P = np.vstack(out) if out else np.zeros((len(texts),4))
        return P

def _load_joblib_if_any():
    try:
        import joblib
        for name in ["tour_theme_regressor.joblib", "tour_theme_regressor_sgd.joblib"]:
            p = Path(MODELS_DIR) / name
            if p.exists():
                return joblib.load(str(p))
        return None
    except Exception:
        return None

from src.config import USE_TORCH

def encode_and_score(texts: List[str]) -> np.ndarray:
    if USE_TORCH:  # ğŸ”˜ í† ì¹˜ ê°•ì œ
        ckpt = _find_torch_model()
        if ckpt:
            try:
                return TorchPredictor(ckpt).predict(texts)
            except Exception as e:
                warnings.warn(f"Torch ì˜ˆì¸¡ ì‹¤íŒ¨: {e} â†’ Sklearn fallback")
        else:
            warnings.warn("Torch ëª¨ë¸ ì—†ìŒ â†’ Sklearn fallback")

    # ğŸ”˜ ê¸°ë³¸: Sklearn
    reg = _load_joblib_if_any()
    if reg is not None:
        try:
            from sentence_transformers import SentenceTransformer
            enc = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
            X = enc.encode(texts, normalize_embeddings=True, convert_to_numpy=True)
            if isinstance(reg, list):
                P = np.column_stack([m.predict(X) for m in reg])
            else:
                P = reg.predict(X)
            return _softmax_rows(P)
        except Exception as e:
            warnings.warn(f"Sklearn ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")

    # ğŸ”˜ ë§ˆì§€ë§‰ ì•ˆì „ë§
    return _heuristic_scores(texts)


def score_and_save(df_or_csv: Union[pd.DataFrame, str],
                   out_csv: Optional[str] = str(SCORED_CSV)) -> pd.DataFrame:
    if isinstance(df_or_csv, str): df = pd.read_csv(df_or_csv)
    else: df = df_or_csv.copy()

    if "embedding_text" not in df.columns:
        raise ValueError("ì…ë ¥ì— 'embedding_text' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. 2ë‹¨ê³„ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")

    texts = df["embedding_text"].fillna("").astype(str).tolist()
    P = encode_and_score(texts)
    P = np.clip(P, 0, None)
    s = P.sum(axis=1, keepdims=True) + 1e-9
    P = P / s

    df["active"], df["culture"], df["nature"], df["relaxation"] = P[:,0], P[:,1], P[:,2], P[:,3]
    if out_csv:
        Path(os.path.dirname(out_csv) or ".").mkdir(parents=True, exist_ok=True)
        df.to_csv(out_csv, index=False, encoding="utf-8-sig")
        print(f"âœ… CSV ì €ì¥: {out_csv} (í–‰:{len(df)})")
    return df

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--input", type=str, default=str(RAW_WITH_TEXTS_CSV))
    ap.add_argument("--output", type=str, default=str(SCORED_CSV))
    a = ap.parse_args()
    score_and_save(a.input, out_csv=a.output)

if __name__ == "__main__":
    main()
