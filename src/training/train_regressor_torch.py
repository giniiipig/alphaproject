
# -*- coding: utf-8 -*-
"""
train_auto_torch.py
-------------------
ì—¬ë– ì¼: "ì‹ ê·œ í•™ìŠµ(ê¸°ë³¸)" vs "ì¶”ê°€ í•™ìŠµ(ì¬í•™ìŠµ)" ìë™ êµ¬ë¶„ ìŠ¤í¬ë¦½íŠ¸

ê·œì¹™
- ê¸°ë³¸ ê²½ë¡œ(models/theme_regressor.pt)ì— ì²´í¬í¬ì¸íŠ¸ê°€ ì—†ìœ¼ë©´ â†’ ì‹ ê·œ í•™ìŠµ
  * train_labels_auto.csv + train_labels_human_input.csv ë³‘í•© â†’ train_labels_merged.csv
  * ë³‘í•©ë³¸ìœ¼ë¡œ í•™ìŠµ
  * ì €ì¥: models/theme_regressor_base.pt (+ ìµœì‹  ëª¨ë¸ ë³„ì¹­: models/theme_regressor.pt ë¡œë„ ì—…ë°ì´íŠ¸)

- ì²´í¬í¬ì¸íŠ¸ê°€ ì´ë¯¸ ìˆê±°ë‚˜, --additional_csv ë¥¼ ì§€ì •í•˜ë©´ â†’ ì¶”ê°€ í•™ìŠµ(ì´ì–´í•™ìŠµ)
  * --additional_csv (ê¸°ë³¸: train_labels_template_for_humans.csvê°€ ìˆìœ¼ë©´ ê·¸ê²ƒ, ì—†ìœ¼ë©´ --train_csv) ë¡œ ì´ì–´í•™ìŠµ
  * (ì˜µì…˜) --replay_csv ë¡œ ê³¼ê±° í° ë°ì´í„° ì¼ë¶€ ì„ê¸°(--replay_ratio)
  * ì €ì¥: models/theme_regressor_additional_YYYYMMDDHHMM.pt (+ ìµœì‹  ëª¨ë¸ ë³„ì¹­ ì—…ë°ì´íŠ¸)

ì„¤ì¹˜
pip install torch sentence-transformers pandas scikit-learn

ì‚¬ìš© ì˜ˆì‹œ
1) ìë™ ëª¨ë“œ(ê¶Œì¥):
python train_auto_torch.py

2) ëª…ì‹œì ìœ¼ë¡œ ì¶”ê°€ í•™ìŠµ(ìƒˆ ì†ŒëŸ‰ ë¼ë²¨ë¡œ ì´ì–´í•˜ê¸°):
python train_auto_torch.py --additional_csv ./train_labels_template_for_humans.csv --epochs 10 --lr 1e-4

3) ì¶”ê°€ í•™ìŠµ + ë¦¬í”Œë ˆì´ í˜¼í•©:
python train_auto_torch.py --additional_csv ./new_small_labels.csv --replay_csv ./train_labels_merged.csv --replay_ratio 0.3 --epochs 12 --lr 1e-4
"""

import os
import argparse
from dataclasses import dataclass
from typing import List, Optional, Tuple
from datetime import datetime

import numpy as np
import pandas as pd

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sentence_transformers import SentenceTransformer


# ===================== ê³µí†µ ìœ í‹¸ =====================

def set_seed(seed: int = 42):
    import random
    random.seed(seed); np.random.seed(seed)
    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)

def safe_prob_norm(y: np.ndarray, eps: float = 1e-9) -> np.ndarray:
    y = np.clip(y, 0, None)
    s = y.sum(axis=-1, keepdims=True) + eps
    return y / s

def ensure_cols(df: pd.DataFrame) -> pd.DataFrame:
    """text, active, culture, nature, relaxation ì»¬ëŸ¼ ê°•ì œ ì •ê·œí™”"""
    colmap = {c.lower(): c for c in df.columns}
    need = ["text","active","culture","nature","relaxation"]
    miss = [c for c in need if c not in colmap]
    if miss:
        raise ValueError(f"CSVì— í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {miss}. ì‹¤ì œ ì»¬ëŸ¼: {list(df.columns)}")
    df = df.rename(columns={
        colmap["text"]: "text",
        colmap["active"]: "active",
        colmap["culture"]: "culture",
        colmap["nature"]: "nature",
        colmap["relaxation"]: "relaxation",
    })
    df["text"] = df["text"].astype(str)
    for c in ["active","culture","nature","relaxation"]:
        df[c] = pd.to_numeric(df[c], errors="coerce").fillna(0.0)
    return df

def merge_auto_human(auto_csv: str, human_csv: str, out_csv: str) -> pd.DataFrame:
    """auto + human_input ë³‘í•©. ì‚¬ëŒì´ ì“´ ê°’ ìš°ì„ , ê²°ì¸¡ì€ auto ê°’ ì‚¬ìš©. í–‰ë³„ í™•ë¥  ì •ê·œí™”."""
    auto = ensure_cols(pd.read_csv(auto_csv))
    human = ensure_cols(pd.read_csv(human_csv))

    # ê¸°ì¤€: text
    merged = auto.merge(human, on="text", how="outer", suffixes=("_auto","_human"))
    out = pd.DataFrame({"text": merged["text"]})
    for c in ["active","culture","nature","relaxation"]:
        ca, ch = f"{c}_auto", f"{c}_human"
        va = merged[ca].fillna(0.0)
        vh = merged[ch].fillna(np.nan)
        # ì‚¬ëŒì´ ì“´ ê°’ ìš°ì„ . ì‚¬ëŒì´ ì—†ìœ¼ë©´ auto ì‚¬ìš©
        v = vh.where(~vh.isna(), va)
        out[c] = v.astype(float)

    # í–‰ë³„ í™•ë¥  ì •ê·œí™”
    P = out[["active","culture","nature","relaxation"]].to_numpy(dtype=float)
    out[["active","culture","nature","relaxation"]] = safe_prob_norm(P)
    out.to_csv(out_csv, index=False, encoding="utf-8-sig")
    print(f"âœ… ë³‘í•© CSV ì €ì¥: {out_csv} (í–‰:{len(out)})")
    return out


# ===================== ë°ì´í„°ì…‹/ëª¨ë¸ =====================

class ThemeDataset(Dataset):
    def __init__(self, texts: List[str], labels: np.ndarray, sbert_name: str):
        self.labels = labels.astype(np.float32)
        self.enc = SentenceTransformer(sbert_name)
        self.X = self.enc.encode(texts, normalize_embeddings=True, convert_to_numpy=True)

    def __len__(self): return len(self.X)
    def __getitem__(self, idx):
        return torch.from_numpy(self.X[idx]).float(), torch.from_numpy(self.labels[idx]).float()

class MLPRegressor(nn.Module):
    def __init__(self, in_dim: int, hidden: int = 512, dropout: float = 0.1):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(in_dim, hidden), nn.ReLU(), nn.Dropout(dropout),
            nn.Linear(hidden, hidden // 2), nn.ReLU(), nn.Dropout(dropout),
            nn.Linear(hidden // 2, 4),
        )
        self.log_softmax = nn.LogSoftmax(dim=-1)
    def forward(self, x):
        return self.log_softmax(self.net(x))


# ===================== í•™ìŠµ ë£¨í”„ =====================

@dataclass
class TrainCfg:
    sbert_name: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    epochs: int = 30
    batch_size: int = 64
    lr: float = 2e-4
    weight_decay: float = 0.0
    hidden: int = 512
    dropout: float = 0.1
    seed: int = 42

def kldiv_loss(log_probs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
    return nn.functional.kl_div(log_probs, targets, reduction="batchmean")

def distribution_metrics(log_probs: torch.Tensor, targets: torch.Tensor) -> Tuple[float, float]:
    with torch.no_grad():
        preds = torch.exp(log_probs)
        mae = torch.mean(torch.abs(preds - targets)).item()
        cos = torch.nn.functional.cosine_similarity(preds, targets, dim=-1).mean().item()
    return mae, cos

def prepare_model(sbert_name: str, hidden: int, dropout: float, device: torch.device) -> Tuple[MLPRegressor, int, SentenceTransformer]:
    enc = SentenceTransformer(sbert_name)
    in_dim = enc.encode(["hello"], normalize_embeddings=True, convert_to_numpy=True).shape[1]
    model = MLPRegressor(in_dim=in_dim, hidden=hidden, dropout=dropout).to(device)
    return model, in_dim, enc

def train_one(df: pd.DataFrame, cfg: TrainCfg,
              resume_ckpt: Optional[str],
              out_ckpt: str) -> str:
    set_seed(cfg.seed)

    df = ensure_cols(df)
    texts = df["text"].tolist()
    labels = safe_prob_norm(df[["active","culture","nature","relaxation"]].to_numpy(dtype=float))

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    # encì€ Dataset ë‚´ë¶€ì—ì„œ ìƒˆë¡œ ë¡œë“œ â†’ ì—¬ê¸°ì„  ì°¨ì› í™•ì¸ìš©ë§Œ ì‚¬ìš©
    sample_enc = SentenceTransformer(cfg.sbert_name)
    in_dim = sample_enc.encode(["x"], normalize_embeddings=True, convert_to_numpy=True).shape[1]

    dataset = ThemeDataset(texts, labels, cfg.sbert_name)
    loader = DataLoader(dataset, batch_size=cfg.batch_size, shuffle=True, num_workers=0)

    model = MLPRegressor(in_dim=in_dim, hidden=cfg.hidden, dropout=cfg.dropout).to(device)
    optim = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)
    best = float("inf")

    # ===== ì¬ê°œ ì˜µì…˜ =====
    if resume_ckpt and os.path.exists(resume_ckpt):
        ckpt = torch.load(resume_ckpt, map_location=device)
        model.load_state_dict(ckpt["state_dict"])
        if "optim_state" in ckpt:
            try:
                optim.load_state_dict(ckpt["optim_state"])
            except Exception:
                pass
        print(f"ğŸ” ì´ì–´ì„œ í•™ìŠµ ì¬ê°œ: {resume_ckpt}")

    for ep in range(1, cfg.epochs + 1):
        model.train()
        tot, n = 0.0, 0
        for xb, yb in loader:
            xb = xb.to(device); yb = yb.to(device)
            optim.zero_grad()
            logp = model(xb)
            loss = kldiv_loss(logp, yb)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optim.step()
            tot += loss.item(); n += 1
        print(f"[Epoch {ep:02d}] train_loss={tot/max(n,1):.4f}")

    # ì €ì¥
    os.makedirs(os.path.dirname(out_ckpt) or ".", exist_ok=True)
    torch.save({
        "state_dict": model.state_dict(),
        "sbert_name": cfg.sbert_name,
        "in_dim": in_dim,
        "hidden": cfg.hidden,
        "dropout": cfg.dropout,
        "optim_state": optim.state_dict(),
    }, out_ckpt)
    print(f"âœ… ëª¨ë¸ ì €ì¥: {out_ckpt}")
    return out_ckpt


# ===================== ë©”ì¸ ë¡œì§ =====================

def file_exists(path: str) -> bool:
    try:
        return os.path.exists(path)
    except Exception:
        return False

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--auto_mode", type=lambda x: str(x).lower() != "false", default=True,
                    help="Trueë©´ ìë™ íŒë‹¨(ê¸°ë³¸: True)")
    ap.add_argument("--model_latest", type=str, default="./models/theme_regressor.pt",
                    help="ìµœì‹  ëª¨ë¸ ë³„ì¹­ ê²½ë¡œ")
    ap.add_argument("--base_ckpt_name", type=str, default="./models/theme_regressor_base.pt",
                    help="ì‹ ê·œ(ê¸°ë³¸) í•™ìŠµ ì €ì¥ ê²½ë¡œ")
    ap.add_argument("--additional_dir", type=str, default="./models",
                    help="ì¶”ê°€í•™ìŠµ ì €ì¥ ë””ë ‰í† ë¦¬")

    # ë°ì´í„°ì…‹ ê²½ë¡œë“¤
    ap.add_argument("--auto_csv", type=str, default="./train_labels_auto.csv")
    ap.add_argument("--human_csv", type=str, default="./train_labels_human_input.csv")
    ap.add_argument("--merged_csv", type=str, default="./train_labels_merged.csv",
                    help="ì‹ ê·œ í•™ìŠµ ì‹œ ìƒì„±ë˜ëŠ” ë³‘í•© CSV ê²½ë¡œ")
    ap.add_argument("--train_csv", type=str, default="./train_labels_human_input.csv",
                    help="ìˆ˜ë™/ê¸°íƒ€ìš© ê¸°ë³¸ í•™ìŠµ ì…ë ¥(ìë™ ëª¨ë“œì—ì„  ë³´í†µ ë³‘í•©ë³¸ ì‚¬ìš©)")
    ap.add_argument("--additional_csv", type=str, default=None,
                    help="ì¶”ê°€í•™ìŠµ ì „ìš© CSV (ê¸°ë³¸: train_labels_template_for_humans.csv ìˆìœ¼ë©´ ê·¸ê²ƒ, ì—†ìœ¼ë©´ --train_csv)")
    ap.add_argument("--replay_csv", type=str, default=None,
                    help="ì¶”ê°€í•™ìŠµ ì‹œ ê³¼ê±° ë°ì´í„° ì¼ë¶€ ì„ì„ ë•Œ ì‚¬ìš©")
    ap.add_argument("--replay_ratio", type=float, default=0.0,
                    help="ì¶”ê°€í•™ìŠµ ì‹œ (ì‹ ê·œ:ë¦¬í”Œë ˆì´) ë¹„ìœ¨; 0ì´ë©´ ë¹„í™œì„±")

    # í•™ìŠµ ì„¤ì •
    ap.add_argument("--epochs", type=int, default=20)
    ap.add_argument("--batch_size", type=int, default=64)
    ap.add_argument("--lr", type=float, default=2e-4)
    ap.add_argument("--weight_decay", type=float, default=0.0)
    ap.add_argument("--hidden", type=int, default=512)
    ap.add_argument("--dropout", type=float, default=0.1)
    ap.add_argument("--seed", type=int, default=42)

    # ìˆ˜ë™ ëª¨ë“œ
    ap.add_argument("--mode", type=str, choices=["fit","continue"], default="fit",
                    help="auto_mode=Falseì—ì„œë§Œ ì‚¬ìš©")

    args = ap.parse_args()

    cfg = TrainCfg(
        epochs=args.epochs, batch_size=args.batch_size, lr=args.lr,
        weight_decay=args.weight_decay, hidden=args.hidden, dropout=args.dropout, seed=args.seed
    )

    # ì¶”ê°€í•™ìŠµ CSV ê¸°ë³¸ê°’(auto-detect)
    if args.additional_csv is None:
        default_add = "./train_labels_template_for_humans.csv"
        args.additional_csv = default_add if file_exists(default_add) else args.train_csv

    # ì˜ì‚¬ê²°ì •: ì‹ ê·œ vs ì¶”ê°€
    if args.auto_mode:
        latest_exists = file_exists(args.model_latest)
        if not latest_exists:
            # ===== ì‹ ê·œ(ê¸°ë³¸) í•™ìŠµ =====
            print("ğŸ†• ì‹ ê·œ(ê¸°ë³¸) í•™ìŠµ ëª¨ë“œ")
            if not (file_exists(args.auto_csv) and file_exists(args.human_csv)):
                raise FileNotFoundError("ì‹ ê·œ í•™ìŠµì—ëŠ” train_labels_auto.csv, train_labels_human_input.csv ê°€ ëª¨ë‘ í•„ìš”í•©ë‹ˆë‹¤.")

            df_merged = merge_auto_human(args.auto_csv, args.human_csv, args.merged_csv)
            out_path = args.base_ckpt_name
            _ = train_one(df_merged, cfg, resume_ckpt=None, out_ckpt=out_path)

            # ìµœì‹  ë³„ì¹­ ì—…ë°ì´íŠ¸
            try:
                os.makedirs(os.path.dirname(args.model_latest) or ".", exist_ok=True)
                # ì‹¬ë³¼ë¦­ ë§í¬ ì‹¤íŒ¨ ì‹œ ë®ì–´ì“°ê¸° ë³µì‚¬
                import shutil
                shutil.copyfile(out_path, args.model_latest)
            except Exception:
                pass
            print(f"ğŸ”— ìµœì‹  ëª¨ë¸ ë³„ì¹­ ì—…ë°ì´íŠ¸: {args.model_latest} -> {out_path}")

        else:
            # ===== ì¶”ê°€ í•™ìŠµ(ì´ì–´í•™ìŠµ) =====
            print("ğŸ” ì¶”ê°€ í•™ìŠµ(ì´ì–´í•™ìŠµ) ëª¨ë“œ")
            # ë² ì´ìŠ¤ë¡œëŠ” ê¸°ì¡´ latest ckpt ì‚¬ìš©
            resume_ckpt = args.model_latest
            # ì…ë ¥ ë°ì´í„° êµ¬ì„±
            df_add = ensure_cols(pd.read_csv(args.additional_csv))

            if args.replay_csv and file_exists(args.replay_csv) and args.replay_ratio > 0:
                df_rep = ensure_cols(pd.read_csv(args.replay_csv))
                # ë¹„ìœ¨ëŒ€ë¡œ ìƒ˜í”Œë§ ê²°í•©
                n_new = len(df_add)
                n_rep = int(n_new * args.replay_ratio / (1 - args.replay_ratio))
                if n_rep > 0 and len(df_rep) > 0:
                    idx = np.random.choice(len(df_rep), size=min(n_rep, len(df_rep)), replace=False)
                    df_add = pd.concat([df_add, df_rep.iloc[idx]], ignore_index=True)
                    print(f"ğŸ“Œ ë¦¬í”Œë ˆì´ ìƒ˜í”Œ ì¶”ê°€: {len(idx)}")

            ts = datetime.now().strftime("%Y%m%d%H%M")
            out_name = f"theme_regressor_additional_{ts}.pt"
            out_path = os.path.join(args.additional_dir, out_name)
            _ = train_one(df_add, cfg, resume_ckpt=resume_ckpt, out_ckpt=out_path)

            # ìµœì‹  ë³„ì¹­ ì—…ë°ì´íŠ¸
            try:
                os.makedirs(os.path.dirname(args.model_latest) or ".", exist_ok=True)
                import shutil
                shutil.copyfile(out_path, args.model_latest)
            except Exception:
                pass
            print(f"ğŸ”— ìµœì‹  ëª¨ë¸ ë³„ì¹­ ì—…ë°ì´íŠ¸: {args.model_latest} -> {out_path}")

    else:
        # ìˆ˜ë™ ëª¨ë“œ
        if args.mode == "fit":
            print("ğŸ†• ìˆ˜ë™: ì‹ ê·œ(ê¸°ë³¸) í•™ìŠµ ëª¨ë“œ")
            df = ensure_cols(pd.read_csv(args.train_csv))
            out_path = args.base_ckpt_name
            _ = train_one(df, cfg, resume_ckpt=None, out_ckpt=out_path)
            import shutil
            try:
                shutil.copyfile(out_path, args.model_latest)
            except Exception:
                pass
        else:
            print("ğŸ” ìˆ˜ë™: ì¶”ê°€ í•™ìŠµ(ì´ì–´í•™ìŠµ) ëª¨ë“œ")
            resume_ckpt = args.model_latest if file_exists(args.model_latest) else args.base_ckpt_name
            df = ensure_cols(pd.read_csv(args.additional_csv))
            ts = datetime.now().strftime("%Y%m%d%H%M")
            out_name = f"theme_regressor_additional_{ts}.pt"
            out_path = os.path.join(args.additional_dir, out_name)
            _ = train_one(df, cfg, resume_ckpt=resume_ckpt, out_ckpt=out_path)
            import shutil
            try:
                shutil.copyfile(out_path, args.model_latest)
            except Exception:
                pass

if __name__ == "__main__":
    main()
